{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BertBiEncoder(nn.Module):\n",
    "    def __init__(self, mention_bert, candidate_bert):\n",
    "        super().__init__()\n",
    "        self.mention_bert = mention_bert\n",
    "        self.candidate_bert = candidate_bert\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, is_mention=True, shard_bsz=None):\n",
    "        if is_mention:\n",
    "            model = self.mention_bert\n",
    "        else:\n",
    "            model = self.candidate_bert\n",
    "            \n",
    "        if shard_bsz is None:\n",
    "            bertrep, _ = model(input_ids, attention_mask=attention_mask)\n",
    "            bertrep = bertrep[:, 0, :]\n",
    "        return bertrep\n",
    "\n",
    "\n",
    "\n",
    "class BertCandidateGenerator(object):\n",
    "    def __init__(self, biencoder, pages, device=\"cpu\"):\n",
    "        self.model = biencoder.to(device)\n",
    "        self.pages = pages\n",
    "        self.device = device\n",
    "        \n",
    "    def train(self,\n",
    "              mention_dataset,\n",
    "              candidate_dataset,\n",
    "              inbatch=True,\n",
    "              lr=1e-5,\n",
    "              batch_size=32,\n",
    "              max_ctxt_len=32\n",
    "             ):\n",
    "        \n",
    "        mention_batch = mention_dataset.batch(batch_size=batch_size, max_ctxt_len=max_ctxt_len)\n",
    "        \n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        \n",
    "        if inbatch:\n",
    "            all_loss = []\n",
    "            steps = 0\n",
    "            for input_ids, labels in mention_batch:\n",
    "                inputs = pad_sequence([torch.LongTensor(token)\n",
    "                                      for token in input_ids], padding_value=0).t().to(self.device)\n",
    "\n",
    "                candidate_input_ids = candidate_dataset.get_pages(labels, max_title_len=50, max_desc_len=100)\n",
    "                candidate_inputs = pad_sequence([torch.LongTensor(token)\n",
    "                                                for token in candidate_input_ids], padding_value=0).t().to(self.device)\n",
    "                \n",
    "                scores = inputs.mm(candidate_inputs.t())\n",
    "                \n",
    "                target = torch.LongTensor(torch.arange(scores.size(1))).to(self.device)\n",
    "                loss = F.cross_entropy(scores, target, reduction=\"mean\")\n",
    "                \n",
    "                all_loss.append(loss.item())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "        return all_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "input_ids = torch.tensor([[0,1,2], [1,2,3]])\n",
    "attention_mask = input_ids > 0\n",
    "\n",
    "mention_bert = AutoModel.from_pretrained('cl-tohoku/bert-base-japanese')\n",
    "candidate_bert = AutoModel.from_pretrained('cl-tohoku/bert-base-japanese')\n",
    "biencoder = BertBiEncoder(mention_bert, candidate_bert)\n",
    "biencoder(input_ids, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}