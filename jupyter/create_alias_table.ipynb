{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "base_dir = Path(\"/data1/ujiie/wiki\")\n",
    "file_set = base_dir.glob(\"**/wiki_*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_id': 3894454, 'title': 'KO_SHIBASAKI_ALL_TIME_BEST', 'is_redirect': False}\n",
      "{'page_id': 3894452, 'title': 'WORLDISTA', 'is_redirect': False}\n",
      "{'page_id': 3894438, 'title': 'シルヴィア・ルウェイン・デイヴィス', 'is_redirect': False}\n",
      "{'page_id': 3894463, 'title': 'ハノイ都市鉄道2A号線', 'is_redirect': False}\n",
      "{'page_id': 3894444, 'title': 'モナ・グルート', 'is_redirect': False}\n",
      "{'page_id': 3894449, 'title': 'ルファック', 'is_redirect': False}\n",
      "{'page_id': 3894464, 'title': '兵部行遠', 'is_redirect': False}\n",
      "{'page_id': 3894445, 'title': '宮澤那名子', 'is_redirect': False}\n",
      "{'page_id': 3894458, 'title': '山脇秀夫', 'is_redirect': False}\n",
      "{'page_id': 3894451, 'title': '時里二郎', 'is_redirect': False}\n",
      "{'page_id': 3894461, 'title': '本宮映画劇場', 'is_redirect': False}\n",
      "{'page_id': 3894450, 'title': '村上信五のスポーツ奇跡の瞬間アワード', 'is_redirect': False}\n",
      "{'page_id': 3894457, 'title': '津島隆太', 'is_redirect': False}\n",
      "{'page_id': 3894465, 'title': '羽鳥×宮本_福岡好いとぉ', 'is_redirect': False}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\nlink_cnt = 0\\nwith open(\"/data1/ujiie/wiki_resource/alias_table.tsv\", \"w\") as fout:\\n    writer = csv.writer(fout)\\n    for fn in file_set:\\n        with open(fn, \\'r\\') as f:\\n            for line in f:\\n                line = line.rstrip()\\n                if not line:\\n                    continue\\n                    \\n                line = json.loads(line)[\"text\"].split(\"\\n\")\\n                line = [unescape(l) for l in line]\\n                line = [l for l in line if \"<a\" in l]\\n                line = \\'\\'.join(line)\\n                matches = re.findall(\\'<a href=\"(.*?)\">(.*?)</a>\\', line)\\n                for m in matches:\\n                    href = unquote_to_bytes(m[0]).decode()\\n                    mention = m[1]\\n                    if href == \"\" or href.startswith(\"http\") or mention == \"\":\\n                        continue\\n                    \\n                    \\n                    if href not in title2id:\\n                        cnt += 1\\n                    else:\\n                        output = (title2id[href], mention)\\n                        if output in alias_set:\\n                            continue\\n                        alias_set.add((title2id[href], mention))\\n                        link_cnt += 1\\n\\n    for page_id, title in list(alias_set):\\n        writer.writerow([page_id, title])\\nprint(redirect_cnt, title_cnt, link_cnt)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create alias table\n",
    "\n",
    "from xml.sax.saxutils import unescape\n",
    "from urllib.parse import unquote_to_bytes\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "\n",
    "title2id = {}\n",
    "id_set = set()\n",
    "alias_set = set()\n",
    "redirect_cnt = 0\n",
    "title_cnt = 0\n",
    "with open('/data1/ujiie/WikiDump/jawiki-20190120-title2pageid.json', 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip()\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        line = json.loads(line)\n",
    "        if line['is_redirect']:\n",
    "            if line['redirect_to']['page_id'] is None:\n",
    "                continue\n",
    "                \n",
    "            if int(line['redirect_to']['page_id']) > 3894437:\n",
    "                print(line)\n",
    "                continue\n",
    "            alias_set.add((line['redirect_to']['page_id'], line['title']))\n",
    "            title2id[line['title']] = line['redirect_to']['page_id']\n",
    "            redirect_cnt += 1\n",
    "        else:\n",
    "            if line['page_id'] is None:\n",
    "                continue\n",
    "            if int(line['page_id']) > 3894437:\n",
    "                print(line)\n",
    "                continue\n",
    "            id_set.add(line['page_id'])\n",
    "            title2id[line['title']] = line['page_id']\n",
    "            alias_set.add((line['page_id'], line['title']))\n",
    "            title_cnt += 1\n",
    "\"\"\" \n",
    "link_cnt = 0\n",
    "with open(\"/data1/ujiie/wiki_resource/alias_table.tsv\", \"w\") as fout:\n",
    "    writer = csv.writer(fout)\n",
    "    for fn in file_set:\n",
    "        with open(fn, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.rstrip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                    \n",
    "                line = json.loads(line)[\"text\"].split(\"\\n\")\n",
    "                line = [unescape(l) for l in line]\n",
    "                line = [l for l in line if \"<a\" in l]\n",
    "                line = ''.join(line)\n",
    "                matches = re.findall('<a href=\"(.*?)\">(.*?)</a>', line)\n",
    "                for m in matches:\n",
    "                    href = unquote_to_bytes(m[0]).decode()\n",
    "                    mention = m[1]\n",
    "                    if href == \"\" or href.startswith(\"http\") or mention == \"\":\n",
    "                        continue\n",
    "                    \n",
    "                    \n",
    "                    if href not in title2id:\n",
    "                        cnt += 1\n",
    "                    else:\n",
    "                        output = (title2id[href], mention)\n",
    "                        if output in alias_set:\n",
    "                            continue\n",
    "                        alias_set.add((title2id[href], mention))\n",
    "                        link_cnt += 1\n",
    "\n",
    "    for page_id, title in list(alias_set):\n",
    "        writer.writerow([page_id, title])\n",
    "print(redirect_cnt, title_cnt, link_cnt)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create id2title table\n",
    "\n",
    "import csv\n",
    "import re\n",
    "import json\n",
    "from xml.sax.saxutils import unescape\n",
    "from urllib.parse import unquote_to_bytes\n",
    "\n",
    "title2id = {}\n",
    "with open(\"/data1/ujiie/wiki_resource/id_title_description.jsonl\", \"w\") as fout:\n",
    "    for fn in file_set:\n",
    "        with open(fn, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.rstrip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                    \n",
    "                \n",
    "                line = json.loads(line)\n",
    "                \n",
    "                # remove redirect page\n",
    "                if int(line['id']) not in id_set:\n",
    "                    continue\n",
    "                \n",
    "                _id = line['id']\n",
    "                title = line['title']\n",
    "                \n",
    "                text = line['text']\n",
    "                text = text.split(\"\\n\")[0]\n",
    "                text = unescape(text)\n",
    "                text = re.sub('<a href=\"(.*?)\">(.*?)</a>', r'\\2', text)\n",
    "                \n",
    "                title2id[line['title']] = line['id']\n",
    "                \n",
    "                fout.write(json.dumps({\"id\": _id, \"title\": title, \"description\": text}) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2366824"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alias_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training data (all)\n",
    "\n",
    "from xml.sax.saxutils import unescape\n",
    "from urllib.parse import unquote_to_bytes\n",
    "import re\n",
    "import json\n",
    "\n",
    "cnt = 0\n",
    "alias_set = set()\n",
    "with open(\"/data1/ujiie/wiki_resource/training_data.jsonl\", \"w\") as fout:\n",
    "    for fn in file_set:\n",
    "        sent = []\n",
    "        with open(fn, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.rstrip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                \n",
    "                line = json.loads(line)\n",
    "                doc = line[\"text\"].split(\"\\n\")\n",
    "                doc = [''] + [unescape(s) for s in doc if s != ''] + [\"\"]\n",
    "                plain_doc = [re.sub('<a href=\"(.*?)\">(.*?)</a>', r'\\2', t) for t in doc]\n",
    "                for idx in range(1, len(doc)-1):\n",
    "                    sent = doc[idx]\n",
    "                    ite = re.finditer('<a href=\"(.*?)\">(.*?)</a>', sent)\n",
    "                    for it in ite:\n",
    "                        link = unquote_to_bytes(it.groups()[0]).decode()\n",
    "                        if link not in title2id:\n",
    "                            continue\n",
    "                        link = title2id[link]\n",
    "                        mention = it.groups()[1]\n",
    "                        start, end = it.span()\n",
    "                        left_context = sent[:start]\n",
    "                        left_context = re.sub('<a href=\"(.*?)\">(.*?)</a>', r'\\2', left_context)\n",
    "                        right_context = sent[end:]\n",
    "                        right_context = re.sub('<a href=\"(.*?)\">(.*?)</a>', r'\\2', right_context)\n",
    "                        output = {\n",
    "                            \"pre_sent\": plain_doc[idx-1],\n",
    "                            \"left_context\": left_context,\n",
    "                            \"right_context\": right_context + plain_doc[idx+1],\n",
    "                            \"post_sent\": plain_doc[idx+1],\n",
    "                            \"mention\": mention,\n",
    "                            \"linkpage_id\": link\n",
    "                        }\n",
    "                        fout.write(json.dumps(output, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create entity prior\n",
    "\n",
    "from xml.sax.saxutils import unescape\n",
    "from urllib.parse import unquote_to_bytes\n",
    "import re\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "cnt = 0\n",
    "\n",
    "entity_prior = defaultdict(int)\n",
    "total = 0\n",
    "\n",
    "for fn in file_set:\n",
    "    with open(fn, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            if not line or line.startswith(\"<doc\") or line.startswith(\"</doc\"):\n",
    "                continue\n",
    "            line = unescape(line)\n",
    "            matches = re.findall('<a href=\"(.*?)\">(.*?)</a>', line)\n",
    "            for m in matches:\n",
    "                href = unquote_to_bytes(m[0]).decode()\n",
    "                mention = m[1]\n",
    "                if href == \"\" or href.startswith(\"http\") or mention == \"\":\n",
    "                    continue\n",
    "                if href not in title2id:\n",
    "                    continue\n",
    "                    \n",
    "                entity_prior[title2id[href]] += 1\n",
    "                total += 1\n",
    "\n",
    "\"\"\"\n",
    "keys = list(entity_prior.keys())\n",
    "for key in keys:\n",
    "    entity_prior[key] = entity_prior[key] / total\n",
    "\"\"\"\n",
    "with open(\"/data1/ujiie/wiki_resource/entity_prior.pkl\", \"wb\") as f:\n",
    "    pickle.dump(entity_prior, f)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mention-entity prior\n",
    "\n",
    "from xml.sax.saxutils import unescape\n",
    "from urllib.parse import unquote_to_bytes\n",
    "import re\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "cnt = 0\n",
    "\n",
    "mention_prior = {}\n",
    "\n",
    "for fn in file_set:\n",
    "    with open(fn, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            line = json.loads(line)[\"text\"].split(\"\\n\")\n",
    "            line = [unescape(l) for l in line]\n",
    "            line = [l for l in line if \"<a\" in l]\n",
    "            line = ''.join(line)\n",
    "            \n",
    "            matches = re.findall('<a href=\"(.*?)\">(.*?)</a>', line)\n",
    "            for m in matches:\n",
    "                href = unquote_to_bytes(m[0]).decode()\n",
    "                mention = m[1]\n",
    "                if href == \"\" or href.startswith(\"http\") or mention == \"\":\n",
    "                    continue\n",
    "                    \n",
    "                if href not in title2id:\n",
    "                    continue\n",
    "                    \n",
    "                if mention not in mention_prior:\n",
    "                    mention_prior[mention] = defaultdict(int)\n",
    "                    \n",
    "                mention_prior[mention][title2id[href]] += 1\n",
    "                \n",
    "for title, _id in title2id.items():\n",
    "        if title not in mention_prior:\n",
    "            mention_prior[title] = defaultdict(int)\n",
    "        mention_prior[title][title2id[title]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(mention_prior.keys())\n",
    "for key in keys:\n",
    "    a = sorted([(a[1], a[0]) for a in mention_prior[key].items()], reverse=True)\n",
    "    mention_prior[key] = a\n",
    "with open(\"/data1/ujiie/wiki_resource/mention_prior.pkl\", \"wb\") as f:\n",
    "    pickle.dump(mention_prior, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1830861"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(title2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, '1375874')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mention_prior[\"工藤優作\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"未来日記-ANOTHER:WORLD-\" in title2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "695:1:Wikipedia:アップロードログ 2004年4月\n"
     ]
    }
   ],
   "source": [
    "path = \"/data1/ujiie/WikiDump/jawiki-20190120-pages-articles-multistream-index.txt\"\n",
    "with open(path, 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip()\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        print(line)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[        0      1858      3715 ... 609247705 609254688 609265353]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "index = np.array([0])\n",
    "with open('/data1/ujiie/wiki_resource/sample_preprocessed.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        index = np.append(index, index[-1] + len(line))\n",
    "index = index[:-1]\n",
    "print(index)\n",
    "np.save('/data1/ujiie/wiki_resource/sample_preprocessed_index.npy', index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"pre_sent\": \"\", \"left_context\": \"\\\\u30a2\\\\u30f3\\\\u30d1\\\\u30b5\\\\u30f3\\\\u30c9 (&\\\\u3001\\\\u82f1\\\\u8a9e\\\\u540d\\\\uff1a) \\\\u3068\\\\u306f\\\\u4e26\\\\u7acb\\\\u52a9\\\\u8a5e\\\\u300c\\\\u2026\\\\u3068\\\\u2026\\\\u300d\\\\u3092\\\\u610f\\\\u5473\\\\u3059\\\\u308b\\\\u8a18\\\\u53f7\\\\u3067\\\\u3042\\\\u308b\\\\u3002\\\\u30e9\\\\u30c6\\\\u30f3\\\\u8a9e\\\\u306e \\\\u306e\", \"right_context\": \"\\\\u3067\\\\u3001Trebuchet MS\\\\u30d5\\\\u30a9\\\\u30f3\\\\u30c8\\\\u3067\\\\u306f\\\\u3001\\\\u3068\\\\u8868\\\\u793a\\\\u3055\\\\u308c \\\\\"et\\\\\" \\\\u306e\\\\u5408\\\\u5b57\\\\u3067\\\\u3042\\\\u308b\\\\u3053\\\\u3068\\\\u304c\\\\u5bb9\\\\u6613\\\\u306b\\\\u308f\\\\u304b\\\\u308b\\\\u3002ampersa\\\\u3001\\\\u3059\\\\u306a\\\\u308f\\\\u3061 \\\\\"and per se and\\\\\"\\\\u3001\\\\u305d\\\\u306e\\\\u610f\\\\u5473\\\\u306f\\\\\"and [the symbol which] by itself [is] and\\\\\"\\\\u3067\\\\u3042\\\\u308b\\\\u3002\\\\u6b74\\\\u53f2.\", \"post_sent\": \"\\\\u6b74\\\\u53f2.\", \"mention\": \"\\\\u5408\\\\u5b57\", \"linkpage_id\": 90949, \"left_ctxt_tokens\": [\"\\\\u30a2\\\\u30f3\", \"##\\\\u30d1\", \"##\\\\u30b5\\\\u30f3\\\\u30c9\", \"(\", \"##&\", \"##\\\\u3001\", \"\\\\u82f1\\\\u8a9e\", \"\\\\u540d\", \":)\", \"\\\\u3068\", \"\\\\u306f\", \"\\\\u4e26\", \"##\\\\u7acb\", \"\\\\u52a9\", \"##\\\\u8a5e\", \"\\\\u300c\", \"##...\", \"\\\\u3068\", \"...\\\\u300d\", \"\\\\u3092\", \"\\\\u610f\\\\u5473\", \"\\\\u3059\\\\u308b\", \"\\\\u8a18\\\\u53f7\", \"\\\\u3067\", \"\\\\u3042\\\\u308b\", \"\\\\u3002\", \"\\\\u30e9\\\\u30c6\\\\u30f3\\\\u8a9e\", \"\\\\u306e\", \"\\\\u306e\"], \"right_ctxt_tokens\": [\"\\\\u3067\", \"\\\\u3001\", \"Tre\", \"##b\", \"##uch\", \"##et\", \"MS\", \"\\\\u30d5\\\\u30a9\\\\u30f3\\\\u30c8\", \"\\\\u3067\", \"\\\\u306f\", \"\\\\u3001\", \"\\\\u3068\", \"\\\\u8868\\\\u793a\", \"\\\\u3055\", \"\\\\u308c\", \"\\\\\"\", \"et\", \"\\\\\"\", \"\\\\u306e\", \"\\\\u5408\", \"\\\\u5b57\", \"\\\\u3067\", \"\\\\u3042\\\\u308b\", \"\\\\u3053\\\\u3068\", \"\\\\u304c\", \"\\\\u5bb9\\\\u6613\", \"\\\\u306b\", \"\\\\u308f\\\\u304b\\\\u308b\", \"\\\\u3002\", \"am\", \"##per\", \"##s\", \"##a\", \"\\\\u3001\", \"\\\\u3059\\\\u306a\\\\u308f\\\\u3061\", \"\\\\\"\", \"and\", \"per\", \"se\", \"and\", \"\\\\\"\\\\u3001\", \"\\\\u305d\\\\u306e\", \"\\\\u610f\\\\u5473\", \"\\\\u306f\", \"\\\\\"\", \"and\", \"[\", \"the\", \"s\", \"##ym\", \"##bo\", \"##l\", \"wh\", \"##ich\", \"]\", \"by\", \"it\", \"##se\", \"##l\", \"##f\", \"[\", \"is\", \"]\", \"and\", \"\\\\\"\", \"\\\\u3067\", \"\\\\u3042\\\\u308b\", \"\\\\u3002\", \"\\\\u6b74\\\\u53f2\", \".\"], \"mention_tokens\": [\"\\\\u5408\", \"\\\\u5b57\"]}'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/data1/ujiie/wiki_resource/sample_preprocessed.jsonl', 'r') as f:\n",
    "    f.seek(index[1])\n",
    "    lines = f.read(index[2] - index[1])[:-1]\n",
    "    \n",
    "    f.seek(index[2])\n",
    "    lines = f.read(index[3] - index[2])[:-1]\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
